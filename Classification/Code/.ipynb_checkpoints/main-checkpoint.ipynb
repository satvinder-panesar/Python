{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Satvinder Singh \n",
    "### Person No: 5024 8888\n",
    "### Project 3: Classification Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, statistics as stats, math, scipy.stats, random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import OrderedDict\n",
    "\n",
    "def evaluate(actual_labels, predictions):\n",
    "    a = 0\n",
    "    b = 0\n",
    "    c = 0\n",
    "    d = 0\n",
    "    for index, prediction in enumerate(predictions):\n",
    "        if prediction == actual_labels[index] and prediction == 0:\n",
    "            d = d + 1\n",
    "        elif prediction == actual_labels[index] and prediction == 1:\n",
    "            a = a + 1\n",
    "        elif prediction == 0 and actual_labels[index] == 1:\n",
    "            b = b + 1\n",
    "        elif prediction == 1 and actual_labels[index] == 0:\n",
    "            c = c + 1      \n",
    "    \n",
    "    accuracy = round((a+d)/(a+b+c+d), 3)\n",
    "    miss_classification_rate = round((b+c)/(a+b+c+d), 3)\n",
    "    \n",
    "    if a+c > 0:\n",
    "        precision = round(a/(a+c), 3)\n",
    "    else:\n",
    "        precision = 0\n",
    "    if a+b > 0:\n",
    "        recall = round(a/(a+b), 3)\n",
    "    else:\n",
    "        recall = 0\n",
    "    if a+b+c > 0:\n",
    "        f_measure = round((2*a)/(2*a+b+c), 3)\n",
    "    else:\n",
    "        f_measure = 0\n",
    "\n",
    "    return accuracy, precision, recall, f_measure, miss_classification_rate\n",
    "    \n",
    "def get_entropy(data):\n",
    "    if len(data) == 0:\n",
    "        return 0\n",
    "    no_of_columns = len(data[0])\n",
    "    no_of_objects = len(data)\n",
    "    no_of_ones = len([x for x in data if x[no_of_columns-1] == 1])\n",
    "    no_of_zeros = no_of_objects - no_of_ones\n",
    "    prob_of_one = no_of_ones / no_of_objects\n",
    "    prob_of_zero = no_of_zeros / no_of_objects\n",
    "    if prob_of_one == 0:\n",
    "        return round(-(prob_of_zero * math.log(prob_of_zero, 2)), 3)\n",
    "    elif prob_of_zero == 0:\n",
    "        return round(-(prob_of_one * math.log(prob_of_one, 2)), 3)\n",
    "    else:\n",
    "        return round(-(prob_of_one * math.log(prob_of_one, 2))-(prob_of_zero * math.log(prob_of_zero, 2)), 3)\n",
    "\n",
    "def build_tree(train_data, data_entropy, columns_with_categorical_data, id, tree_map):\n",
    "    no_of_columns = len(train_data[0])\n",
    "    unique_labels = list(set(train_data[:,no_of_columns-1]))\n",
    "    if len(unique_labels) == 1:\n",
    "        # print(\"all labels are same\")\n",
    "        tree_map[id]=unique_labels[0]\n",
    "        return\n",
    "    attr_selected = -1\n",
    "    threshold_of_attr = -1\n",
    "    max_info_gained = -1\n",
    "    for i in range(0, no_of_columns - 1):      \n",
    "        temp = get_threshold(train_data[:,[i, no_of_columns - 1]], data_entropy, i, columns_with_categorical_data).split(\":\")\n",
    "        temp = [float(x) for x in temp]\n",
    "        info_gained = temp[0]\n",
    "        threshold_value = temp[1]\n",
    "        if max_info_gained == -1:\n",
    "            max_info_gained = info_gained\n",
    "            attr_selected = i\n",
    "            threshold_of_attr = threshold_value\n",
    "        elif info_gained > max_info_gained:\n",
    "            max_info_gained = info_gained\n",
    "            attr_selected = i\n",
    "            threshold_of_attr = threshold_value\n",
    "    # attribute selected is zero based\n",
    "    # print(\"Attribute selected: \"+str(attr_selected)+\"(0-based) Threshold: \"+str(threshold_of_attr))\n",
    "    if attr_selected in columns_with_categorical_data:\n",
    "        left_set = np.array([x for x in train_data if x[attr_selected] == threshold_of_attr])\n",
    "        right_set = np.array([x for x in train_data if x[attr_selected] != threshold_of_attr])\n",
    "        if len(left_set) == 0:\n",
    "            try:\n",
    "                tree_map[id] = stats.mode(right_set[:,len(right_set[0])-1])\n",
    "            except:\n",
    "                tree_map[id] = random.choice(right_set[:,len(right_set[0])-1])\n",
    "        elif len(right_set) == 0:\n",
    "            try:\n",
    "                tree_map[id] = stats.mode(left_set[:,len(left_set[0])-1])\n",
    "            except:\n",
    "                tree_map[id] = random.choice(left_set[:,len(left_set[0])-1])\n",
    "        else:\n",
    "            tree_map[id] = [attr_selected, threshold_of_attr]\n",
    "            build_tree(left_set, data_entropy, columns_with_categorical_data, id+\"1\", tree_map)\n",
    "            build_tree(right_set, data_entropy, columns_with_categorical_data, id+\"2\", tree_map)\n",
    "    else:\n",
    "        left_set = np.array([x for x in train_data if x[attr_selected] < threshold_of_attr])\n",
    "        right_set = np.array([x for x in train_data if x[attr_selected] >= threshold_of_attr])\n",
    "        if len(left_set) == 0:\n",
    "            try:\n",
    "                tree_map[id] = stats.mode(right_set[:,len(right_set[0])-1])\n",
    "            except:\n",
    "                tree_map[id] = random.choice(right_set[:,len(right_set[0])-1])\n",
    "        elif len(right_set) == 0:\n",
    "            try:\n",
    "                tree_map[id] = stats.mode(left_set[:,len(left_set[0])-1])\n",
    "            except:\n",
    "                tree_map[id] = random.choice(left_set[:,len(left_set[0])-1])\n",
    "        else:\n",
    "            tree_map[id] = [attr_selected, threshold_of_attr]\n",
    "            build_tree(left_set, data_entropy, columns_with_categorical_data, id+\"1\", tree_map)\n",
    "            build_tree(right_set, data_entropy, columns_with_categorical_data, id+\"2\", tree_map)\n",
    "    return tree_map\n",
    "\n",
    "def build_tree_2(train_data, no_of_features_to_use, data_entropy, columns_with_categorical_data, id, tree_map, columns_selected_map):\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        no_of_columns = len(train_data[0])\n",
    "\n",
    "        # select columns randomly from train data\n",
    "        columns_selected = random.sample(range(0, no_of_columns - 1), no_of_features_to_use)\n",
    "\n",
    "        local_columns_with_categorical_data = [x for x in columns_with_categorical_data if x in columns_selected]\n",
    "        # append label column\n",
    "        columns_selected.append(no_of_columns - 1)\n",
    "\n",
    "        # get selected columns from train data\n",
    "        local_train_data = train_data[:, columns_selected]\n",
    "\n",
    "        data_entropy = get_entropy(local_train_data)\n",
    "\n",
    "        unique_labels = list(set(local_train_data[:,len(local_train_data[0])-1]))\n",
    "\n",
    "        if len(unique_labels) == 1:\n",
    "            tree_map[id]=unique_labels[0]\n",
    "            return\n",
    "\n",
    "        # if training samples are small or height of tree > number of features\n",
    "        if len(local_train_data) <= 3 or len(id) > no_of_columns:\n",
    "            try:\n",
    "                tree_map[id] = stats.mode(local_train_data[:,len(local_train_data[0])-1])\n",
    "            except:\n",
    "                tree_map[id] = random.choice(local_train_data[:,len(local_train_data[0])-1])\n",
    "            return\n",
    "\n",
    "        attr_selected = -1\n",
    "        threshold_of_attr = -1\n",
    "        max_info_gained = -1\n",
    "\n",
    "        no_of_columns = len(local_train_data[0])\n",
    "\n",
    "        for i in range(0, no_of_columns - 1):      \n",
    "            temp = get_threshold(local_train_data[:,[i, no_of_columns - 1]], data_entropy, i, local_columns_with_categorical_data).split(\":\")\n",
    "            temp = [float(x) for x in temp]\n",
    "            info_gained = temp[0]\n",
    "            threshold_value = temp[1]\n",
    "            if max_info_gained == -1:\n",
    "                max_info_gained = info_gained\n",
    "                attr_selected = i\n",
    "                threshold_of_attr = threshold_value\n",
    "            elif info_gained > max_info_gained:\n",
    "                max_info_gained = info_gained\n",
    "                attr_selected = i\n",
    "                threshold_of_attr = threshold_value\n",
    "        # attribute selected is zero based\n",
    "        attr_selected = columns_selected[attr_selected]\n",
    "        # print(\"Attribute selected: \"+str(attr_selected)+\"(0-based) Threshold: \"+str(threshold_of_attr))\n",
    "        \n",
    "        if attr_selected not in columns_selected_map:\n",
    "            columns_selected_map[attr_selected] = str(threshold_of_attr)\n",
    "            break\n",
    "            \n",
    "        if attr_selected in columns_selected_map:\n",
    "            if attr_selected not in columns_with_categorical_data:\n",
    "                if \":\" not in columns_selected_map[attr_selected]:\n",
    "                    if threshold_of_attr != float(columns_selected_map[attr_selected]):\n",
    "                        columns_selected_map[attr_selected] = columns_selected_map[attr_selected] + \":\" + str(threshold_of_attr)\n",
    "                        break\n",
    "                else:\n",
    "                    temp = columns_selected_map[attr_selected].split(\":\")\n",
    "                    temp = [float(x) for x in temp]\n",
    "                    found = False\n",
    "                    for ele in temp:\n",
    "                        if ele == threshold_of_attr:\n",
    "                            found = True\n",
    "                            break\n",
    "                    if found == False:\n",
    "                        columns_selected_map[attr_selected] = columns_selected_map[attr_selected] + \":\" + str(threshold_of_attr)\n",
    "                        break\n",
    "                \n",
    "    if attr_selected in local_columns_with_categorical_data:\n",
    "        left_set = np.array([x for x in train_data if x[attr_selected] == threshold_of_attr])\n",
    "        right_set = np.array([x for x in train_data if x[attr_selected] != threshold_of_attr])\n",
    "        if len(left_set) == 0:\n",
    "            try:\n",
    "                tree_map[id] = stats.mode(right_set[:,len(right_set[0])-1])\n",
    "            except:\n",
    "                tree_map[id] = random.choice(right_set[:,len(right_set[0])-1])\n",
    "        elif len(right_set) == 0:\n",
    "            try:\n",
    "                tree_map[id] = stats.mode(left_set[:,len(left_set[0])-1])\n",
    "            except:\n",
    "                tree_map[id] = random.choice(left_set[:,len(left_set[0])-1])\n",
    "        else:\n",
    "            tree_map[id] = [attr_selected, threshold_of_attr]\n",
    "            build_tree_2(left_set, no_of_features_to_use, get_entropy(left_set), columns_with_categorical_data, id+\"1\", tree_map, columns_selected_map)\n",
    "            build_tree_2(right_set, no_of_features_to_use, get_entropy(right_set), columns_with_categorical_data, id+\"2\", tree_map, columns_selected_map)\n",
    "    else:\n",
    "        left_set = np.array([x for x in train_data if x[attr_selected] < threshold_of_attr])\n",
    "        right_set = np.array([x for x in train_data if x[attr_selected] >= threshold_of_attr])\n",
    "        if len(left_set) == 0:\n",
    "            try:\n",
    "                tree_map[id] = stats.mode(right_set[:,len(right_set[0])-1])\n",
    "            except:\n",
    "                tree_map[id] = random.choice(right_set[:,len(right_set[0])-1])\n",
    "        elif len(right_set) == 0:\n",
    "            try:\n",
    "                tree_map[id] = stats.mode(left_set[:,len(left_set[0])-1])\n",
    "            except:\n",
    "                tree_map[id] = random.choice(left_set[:,len(left_set[0])-1])\n",
    "        else:\n",
    "            tree_map[id] = [attr_selected, threshold_of_attr]\n",
    "            build_tree_2(left_set, no_of_features_to_use, get_entropy(left_set), columns_with_categorical_data, id+\"1\", tree_map, columns_selected_map)\n",
    "            build_tree_2(right_set, no_of_features_to_use, get_entropy(right_set), columns_with_categorical_data, id+\"2\", tree_map, columns_selected_map)\n",
    "    return tree_map\n",
    "\n",
    "def get_threshold(column, data_entropy, column_index, columns_with_categorical_data):\n",
    "    # column = [column label]\n",
    "    no_of_objects = len(column)\n",
    "    max_info_gained = -1\n",
    "    threshold_value = -1\n",
    "    unique_elements = list(set(column[:,0]))\n",
    "    if column_index in columns_with_categorical_data:\n",
    "        if len(unique_elements) > 1:\n",
    "            for unique_element in unique_elements:\n",
    "                div1 = [x for x in column if x[0] == unique_element]\n",
    "                div2 = [x for x in column if x[0] != unique_element]\n",
    "                entropy_div1 = get_entropy(div1)\n",
    "                entropy_div2 = get_entropy(div2)\n",
    "                info = (len(div1)/no_of_objects*entropy_div1) + (len(div2)/no_of_objects*entropy_div2)\n",
    "                info_gained = data_entropy - info\n",
    "                if max_info_gained == -1:\n",
    "                    max_info_gained = info_gained\n",
    "                    threshold_value = unique_element\n",
    "                elif info_gained > max_info_gained:\n",
    "                    max_info_gained = info_gained\n",
    "                    threshold_value = unique_element\n",
    "            return str(max_info_gained)+\":\"+str(threshold_value)\n",
    "        else:\n",
    "            entropy_div1 = get_entropy(column)\n",
    "            info = (len(column)/no_of_objects*entropy_div1)\n",
    "            info_gained = data_entropy - info\n",
    "            return str(info_gained)+\":\"+str(column[:,0][0])\n",
    "    else:        \n",
    "        for unique_element in unique_elements:\n",
    "            div1 = [x for x in column if x[0] < unique_element]\n",
    "            div2 = [x for x in column if x[0] >= unique_element]\n",
    "            entropy_div1 = get_entropy(div1)\n",
    "            entropy_div2 = get_entropy(div2)\n",
    "            info = (len(div1)/no_of_objects*entropy_div1) + (len(div2)/no_of_objects*entropy_div2)\n",
    "            info_gained = data_entropy - info\n",
    "            if max_info_gained == -1:\n",
    "                max_info_gained = info_gained\n",
    "                threshold_value = unique_element\n",
    "            elif info_gained > max_info_gained:\n",
    "                max_info_gained = info_gained\n",
    "                threshold_value = unique_element\n",
    "        return str(max_info_gained)+\":\"+str(threshold_value)\n",
    "\n",
    "def predict_using_decision_tree(data, tree_map, columns_with_categorical_data):\n",
    "    #getting root node of decision tree\n",
    "    key = \"1.\"\n",
    "    root = tree_map[key]\n",
    "    attribute = root[0]\n",
    "    threshold = root[1]\n",
    "    while True:\n",
    "        if attribute in columns_with_categorical_data:\n",
    "            if data[attribute] == threshold:\n",
    "                # getting left node\n",
    "                key = key + \"1\"\n",
    "                left_node = tree_map[key]\n",
    "                if not isinstance(left_node, list):\n",
    "                    return(left_node)\n",
    "                else:\n",
    "                    attribute = left_node[0]\n",
    "                    threshold = left_node[1]\n",
    "            else:\n",
    "                # getting right node\n",
    "                key = key + \"2\"\n",
    "                right_node = tree_map[key]\n",
    "                if not isinstance(right_node, list):\n",
    "                    return(right_node)\n",
    "                else:\n",
    "                    attribute = right_node[0]\n",
    "                    threshold = right_node[1]                    \n",
    "        else:\n",
    "            if data[attribute] < threshold:\n",
    "                # getting left node\n",
    "                key = key + \"1\"\n",
    "                left_node = tree_map[key]\n",
    "                if not isinstance(left_node, list):\n",
    "                    return(left_node)\n",
    "                else:\n",
    "                    attribute = left_node[0]\n",
    "                    threshold = left_node[1]\n",
    "            else:\n",
    "                # getting right node\n",
    "                key = key + \"2\"\n",
    "                right_node = tree_map[key]\n",
    "                if not isinstance(right_node, list):\n",
    "                    return(right_node)\n",
    "                else:\n",
    "                    attribute = right_node[0]\n",
    "                    threshold = right_node[1]\n",
    "                    \n",
    "def print_decision_tree(tree_map, columns_with_categorical_data, string_to_number_map):\n",
    "    tree_copy = tree_map.copy()\n",
    "    for key in tree_copy:\n",
    "        if(isinstance(tree_copy[key], list)):\n",
    "            value = tree_copy[key]\n",
    "            if value[0] in columns_with_categorical_data:\n",
    "                temp = [x for (x, y) in string_to_number_map.items() if y == value[1]]\n",
    "                tree_copy[key] = [value[0], temp]\n",
    "    print(tree_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====>project3_dataset1.txt\n",
      "Fold: 1\n",
      "Accuracy:0.965\tPrecision: 0.957\tRecall: 0.957\tF-measure: 0.957\n",
      "Fold: 2\n",
      "Accuracy:0.965\tPrecision: 0.941\tRecall: 0.941\tF-measure: 0.941\n",
      "Fold: 3\n",
      "Accuracy:0.965\tPrecision: 0.929\tRecall: 0.929\tF-measure: 0.929\n",
      "Fold: 4\n",
      "Accuracy:0.965\tPrecision: 1.0\tRecall: 0.905\tF-measure: 0.95\n",
      "Fold: 5\n",
      "Accuracy:0.912\tPrecision: 1.0\tRecall: 0.75\tF-measure: 0.857\n",
      "Fold: 6\n",
      "Accuracy:0.982\tPrecision: 1.0\tRecall: 0.963\tF-measure: 0.981\n",
      "Fold: 7\n",
      "Accuracy:1.0\tPrecision: 1.0\tRecall: 1.0\tF-measure: 1.0\n",
      "Fold: 8\n",
      "Accuracy:0.982\tPrecision: 1.0\tRecall: 0.933\tF-measure: 0.966\n",
      "Fold: 9\n",
      "Accuracy:0.947\tPrecision: 1.0\tRecall: 0.903\tF-measure: 0.949\n",
      "Fold: 10\n",
      "Accuracy:0.982\tPrecision: 0.957\tRecall: 1.0\tF-measure: 0.978\n",
      "==>Average metric values:\n",
      "Accuracy:0.966\n",
      "Precision: 0.978\n",
      "Recall: 0.928\n",
      "F-measure: 0.951\n",
      "====>project3_dataset2.txt\n",
      "Fold: 1\n",
      "Accuracy:0.596\tPrecision: 0.556\tRecall: 0.476\tF-measure: 0.513\n",
      "Fold: 2\n",
      "Accuracy:0.766\tPrecision: 0.556\tRecall: 0.769\tF-measure: 0.645\n",
      "Fold: 3\n",
      "Accuracy:0.63\tPrecision: 0.6\tRecall: 0.316\tF-measure: 0.414\n",
      "Fold: 4\n",
      "Accuracy:0.717\tPrecision: 0.583\tRecall: 0.467\tF-measure: 0.519\n",
      "Fold: 5\n",
      "Accuracy:0.565\tPrecision: 0.5\tRecall: 0.1\tF-measure: 0.167\n",
      "Fold: 6\n",
      "Accuracy:0.587\tPrecision: 0.444\tRecall: 0.222\tF-measure: 0.296\n",
      "Fold: 7\n",
      "Accuracy:0.739\tPrecision: 0.556\tRecall: 0.385\tF-measure: 0.455\n",
      "Fold: 8\n",
      "Accuracy:0.696\tPrecision: 0.286\tRecall: 0.182\tF-measure: 0.222\n",
      "Fold: 9\n",
      "Accuracy:0.522\tPrecision: 0.312\tRecall: 0.312\tF-measure: 0.312\n",
      "Fold: 10\n",
      "Accuracy:0.674\tPrecision: 0.462\tRecall: 0.429\tF-measure: 0.444\n",
      "==>Average metric values:\n",
      "Accuracy:0.649\n",
      "Precision: 0.485\n",
      "Recall: 0.366\n",
      "F-measure: 0.399\n"
     ]
    }
   ],
   "source": [
    "filenames = ['project3_dataset1.txt', 'project3_dataset2.txt']\n",
    "\n",
    "for filename in filenames: \n",
    "    print(\"====>\"+filename)\n",
    "    data = pd.read_csv(filename, delimiter=\"\\t\", header=None)\n",
    "    no_of_objects = len(data)\n",
    "    no_of_columns = len(data.columns)\n",
    "    features = np.array(data.iloc[:,0:no_of_columns])\n",
    "    \n",
    "    string_to_number_map = {}\n",
    "    \n",
    "    columns_with_categorical_data = []\n",
    "    \n",
    "    # pre processing\n",
    "    for ele_index, ele in enumerate(features):\n",
    "        for val_index, val in enumerate(ele):\n",
    "            try:\n",
    "                isinstance(float(val), float)\n",
    "            except:\n",
    "                if val_index not in columns_with_categorical_data:\n",
    "                    columns_with_categorical_data.append(val_index)\n",
    "                if val in string_to_number_map:\n",
    "                    ele[val_index] = string_to_number_map[val]\n",
    "                else:\n",
    "                    length = len(string_to_number_map.keys())\n",
    "                    ele[val_index] = length\n",
    "                    string_to_number_map[val] = length\n",
    "        features[ele_index] = ele\n",
    "    features = np.array([[float(y) for y in x] for x in features])\n",
    "    \n",
    "    # normalizing data\n",
    "    means = {}\n",
    "    std_devs = {}\n",
    "    \n",
    "    for i in range(0, no_of_columns - 1):\n",
    "        if i not in columns_with_categorical_data:\n",
    "            means[i] = np.mean(features[:,i])\n",
    "            std_devs[i] = np.std(features[:,i])\n",
    "            \n",
    "    for feature_index, feature in enumerate(features):\n",
    "        for attr_index, ele in enumerate(feature):\n",
    "            if attr_index not in columns_with_categorical_data and attr_index < no_of_columns - 1:\n",
    "                temp = (ele - means[attr_index])/std_devs[attr_index]\n",
    "                feature[attr_index] = temp\n",
    "            features[feature_index] = feature\n",
    "    \n",
    "    no_of_folds = 10\n",
    "    feature_sets = np.array_split(features, no_of_folds)\n",
    "    \n",
    "    k = 5\n",
    "    \n",
    "    acc_accuracy = 0\n",
    "    acc_precision = 0\n",
    "    acc_recall = 0\n",
    "    acc_f_measure = 0\n",
    "    \n",
    "    for i in range(0, no_of_folds):\n",
    "        \n",
    "        print(\"Fold: \"+str(i+1))    \n",
    "        test_data = feature_sets[i]\n",
    "        temp = [x for x in range(0,10) if x !=i]\n",
    "        train_data = feature_sets[temp[0]]\n",
    "        for i in temp[1:len(temp)]:\n",
    "            train_data = np.concatenate((train_data, feature_sets[i]))\n",
    "        \n",
    "        train_labels = train_data[:,no_of_columns-1]\n",
    "        test_labels = test_data[:,no_of_columns-1]\n",
    "        # removing labels from train and test data\n",
    "        train_data = [x[0:no_of_columns-1] for x in train_data]\n",
    "        test_data = [x[0:no_of_columns-1] for x in test_data]\n",
    "        \n",
    "        # predictions\n",
    "        predictions = []\n",
    "        for feature in test_data:\n",
    "            # stores distance-index of nearest neighbors for each test data\n",
    "            distance_index_map = {}\n",
    "            for feature_index, another_feature in enumerate(train_data):\n",
    "                distance = np.subtract(feature, another_feature)\n",
    "                distance = [pow(x, 2) for x in distance]\n",
    "                distance = math.sqrt(sum(distance))\n",
    "                if len(distance_index_map) < k and distance not in distance_index_map:\n",
    "                    distance_index_map[distance] = feature_index\n",
    "                    if len(distance_index_map) == k:\n",
    "                        distance_index_map = OrderedDict(sorted(distance_index_map.items()))\n",
    "                elif len(distance_index_map) >= k and (distance < list(distance_index_map.keys())[0] or distance < list(distance_index_map.keys())[len(distance_index_map) - 1]):\n",
    "                    # deleting last value from dict\n",
    "                    del distance_index_map[list(distance_index_map.keys())[len(distance_index_map)-1]]\n",
    "                    # adding new value\n",
    "                    distance_index_map[distance] = feature_index\n",
    "                    distance_index_map = OrderedDict(sorted(distance_index_map.items()))\n",
    "            nearest_neighbors = []\n",
    "            for key in distance_index_map:\n",
    "                nearest_neighbors.append(distance_index_map[key])\n",
    "            votes = [train_labels[x] for x in nearest_neighbors]\n",
    "            try:\n",
    "                predictions.append(stats.mode(votes))\n",
    "            except:\n",
    "                # >=2 values are repeated equal number of times\n",
    "                # print(\"tie\")\n",
    "                vote_count_map = {}\n",
    "                for vote in votes:\n",
    "                    if vote not in vote_count_map:\n",
    "                        vote_count_map[vote] = 1\n",
    "                    else:\n",
    "                        vote_count_map[vote] = vote_count_map[vote] + 1\n",
    "                if len(vote_count_map) == 2:\n",
    "                    # 2 values are repeated, so get nearest value\n",
    "                    predictions.append(votes[0])\n",
    "                else:\n",
    "                    # sorted dict based on values\n",
    "                    print(\"Tie: more than 2 values are repeated equal number of times\")\n",
    "\n",
    "        accuracy, precision, recall, f_measure, miss_classification_rate = evaluate(test_labels, predictions)\n",
    "        print(\"Accuracy:\"+str(accuracy)+\"\\tPrecision: \"+str(precision)+\"\\tRecall: \"+str(recall)+\"\\tF-measure: \"+str(f_measure))\n",
    "        acc_accuracy += accuracy\n",
    "        acc_precision += precision\n",
    "        acc_recall += recall\n",
    "        acc_f_measure += f_measure\n",
    "        \n",
    "    print(\"==>Average metric values:\")\n",
    "    print(\"Accuracy:\"+str(round(acc_accuracy/no_of_folds, 3)))\n",
    "    print(\"Precision: \"+str(round(acc_precision/no_of_folds, 3)))\n",
    "    print(\"Recall: \"+str(round(acc_recall/no_of_folds, 3)))\n",
    "    print(\"F-measure: \"+str(round(acc_f_measure/no_of_folds, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====>project3_dataset1.txt\n",
      "Fold: 1\n",
      "Accuracy:0.877\tPrecision: 0.864\tRecall: 0.826\tF-measure: 0.844\n",
      "Fold: 2\n",
      "Accuracy:0.895\tPrecision: 0.789\tRecall: 0.882\tF-measure: 0.833\n",
      "Fold: 3\n",
      "Accuracy:1.0\tPrecision: 1.0\tRecall: 1.0\tF-measure: 1.0\n",
      "Fold: 4\n",
      "Accuracy:0.947\tPrecision: 0.909\tRecall: 0.952\tF-measure: 0.93\n",
      "Fold: 5\n",
      "Accuracy:0.86\tPrecision: 0.8\tRecall: 0.8\tF-measure: 0.8\n",
      "Fold: 6\n",
      "Accuracy:0.965\tPrecision: 1.0\tRecall: 0.926\tF-measure: 0.962\n",
      "Fold: 7\n",
      "Accuracy:0.93\tPrecision: 0.875\tRecall: 0.955\tF-measure: 0.913\n",
      "Fold: 8\n",
      "Accuracy:0.965\tPrecision: 0.933\tRecall: 0.933\tF-measure: 0.933\n",
      "Fold: 9\n",
      "Accuracy:0.877\tPrecision: 0.929\tRecall: 0.839\tF-measure: 0.881\n",
      "Fold: 10\n",
      "Accuracy:0.893\tPrecision: 0.864\tRecall: 0.864\tF-measure: 0.864\n",
      "==>Average metric values:\n",
      "Accuracy:0.921\n",
      "Precision: 0.896\n",
      "Recall: 0.898\n",
      "F-measure: 0.896\n",
      "====>project3_dataset2.txt\n",
      "Fold: 1\n",
      "Accuracy:0.532\tPrecision: 0.474\tRecall: 0.429\tF-measure: 0.45\n",
      "Fold: 2\n",
      "Accuracy:0.702\tPrecision: 0.474\tRecall: 0.692\tF-measure: 0.562\n",
      "Fold: 3\n",
      "Accuracy:0.522\tPrecision: 0.412\tRecall: 0.368\tF-measure: 0.389\n",
      "Fold: 4\n",
      "Accuracy:0.522\tPrecision: 0.316\tRecall: 0.4\tF-measure: 0.353\n",
      "Fold: 5\n",
      "Accuracy:0.565\tPrecision: 0.5\tRecall: 0.4\tF-measure: 0.444\n",
      "Fold: 6\n",
      "Accuracy:0.543\tPrecision: 0.412\tRecall: 0.389\tF-measure: 0.4\n",
      "Fold: 7\n",
      "Accuracy:0.587\tPrecision: 0.286\tRecall: 0.308\tF-measure: 0.296\n",
      "Fold: 8\n",
      "Accuracy:0.739\tPrecision: 0.455\tRecall: 0.455\tF-measure: 0.455\n",
      "Fold: 9\n",
      "Accuracy:0.609\tPrecision: 0.429\tRecall: 0.375\tF-measure: 0.4\n",
      "Fold: 10\n",
      "Accuracy:0.674\tPrecision: 0.467\tRecall: 0.5\tF-measure: 0.483\n",
      "==>Average metric values:\n",
      "Accuracy:0.6\n",
      "Precision: 0.422\n",
      "Recall: 0.432\n",
      "F-measure: 0.423\n"
     ]
    }
   ],
   "source": [
    "filenames = ['project3_dataset1.txt', 'project3_dataset2.txt']\n",
    "    \n",
    "for filename in filenames: \n",
    "    print(\"====>\"+filename)\n",
    "    data = pd.read_csv(filename, delimiter=\"\\t\", header=None)\n",
    "    no_of_columns = len(data.columns)\n",
    "    features = np.array(data.iloc[:,0:no_of_columns])\n",
    "    features = np.array([list(x) for x in features])\n",
    "    \n",
    "    string_to_number_map = {}\n",
    "    columns_with_categorical_data = []\n",
    "    \n",
    "    # pre processing\n",
    "    for ele_index, ele in enumerate(features):\n",
    "        for val_index, val in enumerate(ele):\n",
    "            try:\n",
    "                isinstance(float(val), float)\n",
    "            except:\n",
    "                if val_index not in columns_with_categorical_data:\n",
    "                    columns_with_categorical_data.append(val_index)\n",
    "                if val in string_to_number_map:\n",
    "                    ele[val_index] = string_to_number_map[val]\n",
    "                else:\n",
    "                    length = len(string_to_number_map.keys())\n",
    "                    ele[val_index] = length\n",
    "                    string_to_number_map[val] = length\n",
    "        features[ele_index] = ele\n",
    "    features = np.array([[float(y) for y in x] for x in features])\n",
    "    \n",
    "    no_of_folds = 10\n",
    "    feature_sets = np.array_split(features, no_of_folds)\n",
    "    \n",
    "    acc_accuracy = 0\n",
    "    acc_precision = 0\n",
    "    acc_recall = 0\n",
    "    acc_f_measure = 0\n",
    "    \n",
    "    for i in range(0, no_of_folds):\n",
    "        \n",
    "        print(\"Fold: \"+str(i+1))    \n",
    "        test_data = feature_sets[i]\n",
    "        temp = [x for x in range(0,10) if x !=i]\n",
    "        train_data = feature_sets[temp[0]]\n",
    "        for i in temp[1:len(temp)]:\n",
    "            train_data = np.concatenate((train_data, feature_sets[i]))\n",
    "        \n",
    "        train_labels = train_data[:,no_of_columns-1]\n",
    "        test_labels = test_data[:,no_of_columns-1]\n",
    "        # removing labels from test data\n",
    "        test_data = [x[0:no_of_columns-1] for x in test_data]\n",
    "        data_entropy = get_entropy(train_data)\n",
    "        \n",
    "        # construct decision tree\n",
    "        tree_map = build_tree(train_data, data_entropy, columns_with_categorical_data, \"1.\", {})\n",
    "        \n",
    "        predictions = []\n",
    "                \n",
    "        local_test_data = [x[0:no_of_columns-1] for x in train_data]\n",
    "        for data in local_test_data:\n",
    "                predictions.append(predict_using_decision_tree(data, tree_map, columns_with_categorical_data))\n",
    "\n",
    "        accuracy, precision, recall, f_measure, miss_classification_rate = evaluate(train_labels, predictions)\n",
    "\n",
    "        if int(accuracy) == 1: \n",
    "            # post pruning\n",
    "            while True:            \n",
    "                leaf_nodes = [x for x in tree_map if tree_map[x] == 1 or tree_map[x] == 0]\n",
    "                level = [len(x) for x in leaf_nodes]\n",
    "                max_level = level.index(max(level))\n",
    "                node_to_prune = leaf_nodes[max_level]\n",
    "                parent_node = node_to_prune[0:len(node_to_prune)-1]\n",
    "                if parent_node == \"1.\":\n",
    "                    break\n",
    "                leaf_node_value = tree_map[node_to_prune]\n",
    "                del tree_map[node_to_prune]\n",
    "                tree_map[parent_node] = leaf_node_value\n",
    "\n",
    "                predictions = []\n",
    "\n",
    "                for data in local_test_data:\n",
    "                    predictions.append(predict_using_decision_tree(data, tree_map, columns_with_categorical_data))\n",
    "\n",
    "                accuracy, precision, recall, f_measure, miss_classification_rate = evaluate(train_labels, predictions)\n",
    "\n",
    "                if accuracy < 1:\n",
    "                    break\n",
    "                    \n",
    "        # print_decision_tree(tree_map, columns_with_categorical_data, string_to_number_map)\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for data in test_data:\n",
    "            predictions.append(predict_using_decision_tree(data, tree_map, columns_with_categorical_data))\n",
    "\n",
    "        accuracy, precision, recall, f_measure, miss_classification_rate = evaluate(test_labels, predictions)\n",
    "        print(\"Accuracy:\"+str(accuracy)+\"\\tPrecision: \"+str(precision)+\"\\tRecall: \"+str(recall)+\"\\tF-measure: \"+str(f_measure))\n",
    "        acc_accuracy += accuracy\n",
    "        acc_precision += precision\n",
    "        acc_recall += recall\n",
    "        acc_f_measure += f_measure\n",
    "        \n",
    "    print(\"==>Average metric values:\")\n",
    "    print(\"Accuracy:\"+str(round(acc_accuracy/no_of_folds, 3)))\n",
    "    print(\"Precision: \"+str(round(acc_precision/no_of_folds, 3)))\n",
    "    print(\"Recall: \"+str(round(acc_recall/no_of_folds, 3)))\n",
    "    print(\"F-measure: \"+str(round(acc_f_measure/no_of_folds, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====>project3_dataset1.txt\n",
      "Fold: 1\n",
      "Accuracy:0.947\tPrecision: 0.917\tRecall: 0.957\tF-measure: 0.936\n",
      "Fold: 2\n",
      "Accuracy:0.93\tPrecision: 0.882\tRecall: 0.882\tF-measure: 0.882\n",
      "Fold: 3\n",
      "Accuracy:0.965\tPrecision: 0.875\tRecall: 1.0\tF-measure: 0.933\n",
      "Fold: 4\n",
      "Accuracy:0.912\tPrecision: 0.9\tRecall: 0.857\tF-measure: 0.878\n",
      "Fold: 5\n",
      "Accuracy:0.895\tPrecision: 0.889\tRecall: 0.8\tF-measure: 0.842\n",
      "Fold: 6\n",
      "Accuracy:0.965\tPrecision: 1.0\tRecall: 0.926\tF-measure: 0.962\n",
      "Fold: 7\n",
      "Accuracy:0.965\tPrecision: 0.955\tRecall: 0.955\tF-measure: 0.955\n",
      "Fold: 8\n",
      "Accuracy:0.965\tPrecision: 0.933\tRecall: 0.933\tF-measure: 0.933\n",
      "Fold: 9\n",
      "Accuracy:0.912\tPrecision: 0.964\tRecall: 0.871\tF-measure: 0.915\n",
      "Fold: 10\n",
      "Accuracy:0.893\tPrecision: 0.864\tRecall: 0.864\tF-measure: 0.864\n",
      "==>Average metric values:\n",
      "Accuracy:0.935\n",
      "Precision: 0.918\n",
      "Recall: 0.904\n",
      "F-measure: 0.91\n",
      "====>project3_dataset2.txt\n",
      "Fold: 1\n",
      "Accuracy:0.66\tPrecision: 0.6\tRecall: 0.714\tF-measure: 0.652\n",
      "Fold: 2\n",
      "Accuracy:0.745\tPrecision: 0.524\tRecall: 0.846\tF-measure: 0.647\n",
      "Fold: 3\n",
      "Accuracy:0.783\tPrecision: 0.765\tRecall: 0.684\tF-measure: 0.722\n",
      "Fold: 4\n",
      "Accuracy:0.674\tPrecision: 0.5\tRecall: 0.6\tF-measure: 0.545\n",
      "Fold: 5\n",
      "Accuracy:0.63\tPrecision: 0.6\tRecall: 0.45\tF-measure: 0.514\n",
      "Fold: 6\n",
      "Accuracy:0.63\tPrecision: 0.526\tRecall: 0.556\tF-measure: 0.541\n",
      "Fold: 7\n",
      "Accuracy:0.848\tPrecision: 0.75\tRecall: 0.692\tF-measure: 0.72\n",
      "Fold: 8\n",
      "Accuracy:0.761\tPrecision: 0.5\tRecall: 0.545\tF-measure: 0.522\n",
      "Fold: 9\n",
      "Accuracy:0.609\tPrecision: 0.444\tRecall: 0.5\tF-measure: 0.471\n",
      "Fold: 10\n",
      "Accuracy:0.696\tPrecision: 0.5\tRecall: 0.571\tF-measure: 0.533\n",
      "==>Average metric values:\n",
      "Accuracy:0.704\n",
      "Precision: 0.571\n",
      "Recall: 0.616\n",
      "F-measure: 0.587\n"
     ]
    }
   ],
   "source": [
    "filenames = ['project3_dataset1.txt', 'project3_dataset2.txt']\n",
    "\n",
    "for filename in filenames: \n",
    "    print(\"====>\"+filename)\n",
    "    data = pd.read_csv(filename, delimiter=\"\\t\", header=None)\n",
    "    no_of_columns = len(data.columns)\n",
    "    features = np.array(data.iloc[:,0:no_of_columns])\n",
    "    features = np.array([list(x) for x in features])\n",
    "    \n",
    "    string_to_number_map = {}    \n",
    "    columns_with_categorical_data = []\n",
    "    \n",
    "    # pre processing\n",
    "    for ele_index, ele in enumerate(features):\n",
    "        for val_index, val in enumerate(ele):\n",
    "            try:\n",
    "                isinstance(float(val), float)\n",
    "            except:\n",
    "                if val_index not in columns_with_categorical_data:\n",
    "                    columns_with_categorical_data.append(val_index)\n",
    "                if val in string_to_number_map:\n",
    "                    ele[val_index] = string_to_number_map[val]\n",
    "                else:\n",
    "                    length = len(string_to_number_map.keys())\n",
    "                    ele[val_index] = length\n",
    "                    string_to_number_map[val] = length\n",
    "        features[ele_index] = ele\n",
    "    features = np.array([[float(y) for y in x] for x in features])\n",
    "            \n",
    "    no_of_folds = 10\n",
    "    feature_sets = np.array_split(features, no_of_folds)\n",
    "    \n",
    "    acc_accuracy = 0\n",
    "    acc_precision = 0\n",
    "    acc_recall = 0\n",
    "    acc_f_measure = 0\n",
    "    \n",
    "    for i in range(0, no_of_folds):\n",
    "        \n",
    "        print(\"Fold: \"+str(i+1))    \n",
    "        test_data = feature_sets[i]\n",
    "        temp = [x for x in range(0,10) if x !=i]\n",
    "        train_data = feature_sets[temp[0]]\n",
    "        for i in temp[1:len(temp)]:\n",
    "            train_data = np.concatenate((train_data, feature_sets[i]))        \n",
    "        no_of_objects = len(train_data)\n",
    "        \n",
    "        train_labels = train_data[:,no_of_columns-1]\n",
    "        train_labels = [float(x) for x in train_labels]\n",
    "        test_labels = test_data[:,no_of_columns-1]\n",
    "        test_labels = [float(x) for x in test_labels]\n",
    "        # removing labels from test data\n",
    "        test_data = [x[0:no_of_columns-1] for x in test_data]\n",
    "        \n",
    "        # stores probability of 0 and 1\n",
    "        class_prior_probabilities = []        \n",
    "        \n",
    "        for ele in set(train_labels):\n",
    "            class_prior_probabilities.append(len([x for x in train_labels if x == ele])/len(train_labels))\n",
    "            \n",
    "        labels = list(set(train_labels))\n",
    "        labels = [int(x) for x in labels]\n",
    "        labels_count = []\n",
    "        \n",
    "        for label in labels:\n",
    "            labels_count.append(len([x for x in train_labels if x == label]))\n",
    "    \n",
    "        # stores [mean, std] for 0 and 1 of all columns\n",
    "        mean_std_dev = []\n",
    "        for i in range(0, len(train_data[0]) -1):\n",
    "            temp = []\n",
    "            for label in labels:\n",
    "                data = [x[i] for x in train_data if x[-1] == label]\n",
    "                temp.append([np.mean(data), np.std(data)])\n",
    "            mean_std_dev.append(temp)\n",
    "\n",
    "        predictions = []\n",
    "        \n",
    "        for data in test_data:\n",
    "            final_probabilities = []\n",
    "            intermediate_probabilities = []\n",
    "            # descriptor prior\n",
    "            den = no_of_objects\n",
    "            for ele_index, ele in enumerate(data):\n",
    "                corrected_using_laplacian = 0\n",
    "                num = len([x for x in train_data if x[ele_index] == ele])\n",
    "                if num == 0:\n",
    "                    # laplacian correction\n",
    "                    num = 1\n",
    "                    corrected_using_laplacian = 1\n",
    "                intermediate_probabilities.append(num/(den + corrected_using_laplacian))\n",
    "            descriptor_prior_probability = np.product(intermediate_probabilities)\n",
    "            for label_index, label in enumerate(labels):\n",
    "                den = labels_count[label]\n",
    "                intermediate_probabilities = []\n",
    "                class_prior_probability = class_prior_probabilities[label]\n",
    "                den = labels_count[label]\n",
    "                for ele_index, ele in enumerate(data):\n",
    "                    if ele_index not in columns_with_categorical_data:\n",
    "                        temp = mean_std_dev[ele_index]\n",
    "                        temp = temp[label_index]\n",
    "                        # representing random variables as a normal distibution and using pdf\n",
    "                        intermediate_probabilities.append(scipy.stats.norm(temp[0], temp[1]).pdf(ele))\n",
    "                    else:\n",
    "                        num = len([x for x in train_data if x[ele_index] == ele and x[-1] == label])\n",
    "                        intermediate_probabilities.append(num/den)\n",
    "                posterior_probability = np.product(intermediate_probabilities)\n",
    "                final_probabilities.append((posterior_probability * class_prior_probability)/descriptor_prior_probability)\n",
    "                index = final_probabilities.index(max(final_probabilities))\n",
    "            predictions.append(labels[index])\n",
    "            \n",
    "        accuracy, precision, recall, f_measure, miss_classification_rate = evaluate(test_labels, predictions)\n",
    "        print(\"Accuracy:\"+str(accuracy)+\"\\tPrecision: \"+str(precision)+\"\\tRecall: \"+str(recall)+\"\\tF-measure: \"+str(f_measure))\n",
    "        acc_accuracy += accuracy\n",
    "        acc_precision += precision\n",
    "        acc_recall += recall\n",
    "        acc_f_measure += f_measure\n",
    "    \n",
    "    print(\"==>Average metric values:\")\n",
    "    print(\"Accuracy:\"+str(round(acc_accuracy/no_of_folds, 3)))\n",
    "    print(\"Precision: \"+str(round(acc_precision/no_of_folds, 3)))\n",
    "    print(\"Recall: \"+str(round(acc_recall/no_of_folds, 3)))\n",
    "    print(\"F-measure: \"+str(round(acc_f_measure/no_of_folds, 3)))                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====>project3_dataset1.txt\n",
      "Fold: 1\n",
      "Accuracy:0.947\tPrecision: 0.917\tRecall: 0.957\tF-measure: 0.936\n",
      "Fold: 2\n",
      "Accuracy:0.93\tPrecision: 0.882\tRecall: 0.882\tF-measure: 0.882\n",
      "Fold: 3\n",
      "Accuracy:0.965\tPrecision: 1.0\tRecall: 0.857\tF-measure: 0.923\n",
      "Fold: 4\n",
      "Accuracy:0.947\tPrecision: 0.909\tRecall: 0.952\tF-measure: 0.93\n",
      "Fold: 5\n",
      "Accuracy:0.947\tPrecision: 1.0\tRecall: 0.85\tF-measure: 0.919\n",
      "Fold: 6\n",
      "Accuracy:0.93\tPrecision: 0.96\tRecall: 0.889\tF-measure: 0.923\n",
      "Fold: 7\n",
      "Accuracy:0.965\tPrecision: 0.917\tRecall: 1.0\tF-measure: 0.957\n",
      "Fold: 8\n",
      "Accuracy:0.965\tPrecision: 1.0\tRecall: 0.867\tF-measure: 0.929\n",
      "Fold: 9\n",
      "Accuracy:0.947\tPrecision: 0.967\tRecall: 0.935\tF-measure: 0.951\n",
      "Fold: 10\n",
      "Accuracy:0.946\tPrecision: 1.0\tRecall: 0.864\tF-measure: 0.927\n",
      "==>Average metric values:\n",
      "Accuracy:0.949\n",
      "Precision: 0.955\n",
      "Recall: 0.905\n",
      "F-measure: 0.928\n",
      "====>project3_dataset2.txt\n",
      "Fold: 1\n",
      "Accuracy:0.468\tPrecision: 0.3\tRecall: 0.143\tF-measure: 0.194\n",
      "Fold: 2\n",
      "Accuracy:0.723\tPrecision: 0.5\tRecall: 0.462\tF-measure: 0.48\n",
      "Fold: 3\n",
      "Accuracy:0.609\tPrecision: 0.6\tRecall: 0.158\tF-measure: 0.25\n",
      "Fold: 4\n",
      "Accuracy:0.609\tPrecision: 0.4\tRecall: 0.4\tF-measure: 0.4\n",
      "Fold: 5\n",
      "Accuracy:0.587\tPrecision: 0.6\tRecall: 0.15\tF-measure: 0.24\n",
      "Fold: 6\n",
      "Accuracy:0.522\tPrecision: 0.333\tRecall: 0.222\tF-measure: 0.267\n",
      "Fold: 7\n",
      "Accuracy:0.717\tPrecision: 0.5\tRecall: 0.385\tF-measure: 0.435\n",
      "Fold: 8\n",
      "Accuracy:0.717\tPrecision: 0.417\tRecall: 0.455\tF-measure: 0.435\n",
      "Fold: 9\n",
      "Accuracy:0.587\tPrecision: 0.364\tRecall: 0.25\tF-measure: 0.296\n",
      "Fold: 10\n",
      "Accuracy:0.674\tPrecision: 0.444\tRecall: 0.286\tF-measure: 0.348\n",
      "==>Average metric values:\n",
      "Accuracy:0.621\n",
      "Precision: 0.446\n",
      "Recall: 0.291\n",
      "F-measure: 0.334\n"
     ]
    }
   ],
   "source": [
    "filenames = ['project3_dataset1.txt', 'project3_dataset2.txt']\n",
    "    \n",
    "for filename in filenames: \n",
    "    print(\"====>\"+filename)\n",
    "    data = pd.read_csv(filename, delimiter=\"\\t\", header=None)\n",
    "    no_of_columns = len(data.columns)\n",
    "    features = np.array(data.iloc[:,0:no_of_columns])\n",
    "    features = np.array([list(x) for x in features])\n",
    "    \n",
    "    string_to_number_map = {}\n",
    "    columns_with_categorical_data = []\n",
    "    \n",
    "    # pre processing\n",
    "    for ele_index, ele in enumerate(features):\n",
    "        for val_index, val in enumerate(ele):\n",
    "            try:\n",
    "                isinstance(float(val), float)\n",
    "            except:\n",
    "                if val_index not in columns_with_categorical_data:\n",
    "                    columns_with_categorical_data.append(val_index)\n",
    "                if val in string_to_number_map:\n",
    "                    ele[val_index] = string_to_number_map[val]\n",
    "                else:\n",
    "                    length = len(string_to_number_map.keys())\n",
    "                    ele[val_index] = length\n",
    "                    string_to_number_map[val] = length\n",
    "        features[ele_index] = ele\n",
    "    features = np.array([[float(y) for y in x] for x in features])\n",
    "    \n",
    "    no_of_folds = 10\n",
    "    feature_sets = np.array_split(features, no_of_folds)\n",
    "    \n",
    "    acc_accuracy = 0\n",
    "    acc_precision = 0\n",
    "    acc_recall = 0\n",
    "    acc_f_measure = 0\n",
    "    \n",
    "    no_of_decision_trees = 5\n",
    "    no_of_features_to_use = math.ceil(0.2 * (no_of_columns - 1))\n",
    "    \n",
    "    for i in range(0, no_of_folds):\n",
    "        \n",
    "        print(\"Fold: \"+str(i+1))    \n",
    "        test_data = feature_sets[i]\n",
    "        temp = [x for x in range(0,10) if x !=i]\n",
    "        train_data = feature_sets[temp[0]]\n",
    "        for i in temp[1:len(temp)]:\n",
    "            train_data = np.concatenate((train_data, feature_sets[i]))\n",
    "        \n",
    "        test_labels = test_data[:,no_of_columns-1]\n",
    "        # removing labels from test data\n",
    "        test_data = [x[0:no_of_columns-1] for x in test_data]\n",
    "        \n",
    "        trees = []\n",
    "            \n",
    "        for i in range(0, no_of_decision_trees):\n",
    "            \n",
    "            min_miss_classification_rate = -1\n",
    "            best_tree_map = {}\n",
    "            \n",
    "            data = []\n",
    "            \n",
    "            # sampling with replacement\n",
    "            indices = np.random.choice(len(train_data), int(len(train_data) / no_of_decision_trees), replace = True)\n",
    "            for index in indices:\n",
    "                data.append(train_data[index])\n",
    "            \n",
    "            temp = np.array_split(data, 3)\n",
    "            local_train_data = np.concatenate((temp[0], temp[1]))\n",
    "            local_test_data = temp[2]\n",
    "            \n",
    "            local_test_labels = local_test_data[:,no_of_columns-1]\n",
    "            # removing labels from local test data\n",
    "            local_test_data = [x[0:no_of_columns-1] for x in local_test_data]\n",
    "            \n",
    "            # repeat n times to get best columns set\n",
    "            for iteration in range(0, 20):\n",
    "            \n",
    "                # construct decision tree\n",
    "                tree_map = build_tree_2(local_train_data, no_of_features_to_use, get_entropy(local_train_data), columns_with_categorical_data, \"1.\", {}, {})\n",
    "                \n",
    "                predictions = []\n",
    "\n",
    "                for data in local_test_data:\n",
    "                    predictions.append(predict_using_decision_tree(data, tree_map, columns_with_categorical_data))\n",
    "\n",
    "                accuracy, precision, recall, f_measure, miss_classification_rate = evaluate(local_test_labels, predictions)\n",
    "                \n",
    "                if min_miss_classification_rate == -1:\n",
    "                    min_miss_classification_rate = miss_classification_rate\n",
    "                    best_tree_map = tree_map.copy()\n",
    "                else:\n",
    "                    if miss_classification_rate < min_miss_classification_rate:\n",
    "                        min_miss_classification_rate = miss_classification_rate\n",
    "                        best_tree_map = tree_map.copy()\n",
    "                \n",
    "            trees.append(best_tree_map)\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        for data in test_data:\n",
    "            different_predictions = []\n",
    "            for i in range(0, no_of_decision_trees):\n",
    "                different_predictions.append(predict_using_decision_tree(data, trees[i], columns_with_categorical_data))\n",
    "            predictions.append(stats.mode(different_predictions))\n",
    "\n",
    "        accuracy, precision, recall, f_measure, miss_classification_rate = evaluate(test_labels, predictions)\n",
    "        print(\"Accuracy:\"+str(accuracy)+\"\\tPrecision: \"+str(precision)+\"\\tRecall: \"+str(recall)+\"\\tF-measure: \"+str(f_measure))\n",
    "        acc_accuracy += accuracy\n",
    "        acc_precision += precision\n",
    "        acc_recall += recall\n",
    "        acc_f_measure += f_measure\n",
    "        \n",
    "    print(\"==>Average metric values:\")\n",
    "    print(\"Accuracy:\"+str(round(acc_accuracy/no_of_folds, 3)))\n",
    "    print(\"Precision: \"+str(round(acc_precision/no_of_folds, 3)))\n",
    "    print(\"Recall: \"+str(round(acc_recall/no_of_folds, 3)))\n",
    "    print(\"F-measure: \"+str(round(acc_f_measure/no_of_folds, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====>project3_dataset1.txt\n",
      "Fold: 1\n",
      "Accuracy:0.965\tPrecision: 0.957\tRecall: 0.957\tF-measure: 0.957\n",
      "Fold: 2\n",
      "Accuracy:0.982\tPrecision: 1.0\tRecall: 0.941\tF-measure: 0.97\n",
      "Fold: 3\n",
      "Accuracy:0.965\tPrecision: 1.0\tRecall: 0.857\tF-measure: 0.923\n",
      "Fold: 4\n",
      "Accuracy:0.965\tPrecision: 1.0\tRecall: 0.905\tF-measure: 0.95\n",
      "Fold: 5\n",
      "Accuracy:0.93\tPrecision: 0.944\tRecall: 0.85\tF-measure: 0.895\n",
      "Fold: 6\n",
      "Accuracy:0.895\tPrecision: 0.957\tRecall: 0.815\tF-measure: 0.88\n",
      "Fold: 7\n",
      "Accuracy:0.965\tPrecision: 0.917\tRecall: 1.0\tF-measure: 0.957\n",
      "Fold: 8\n",
      "Accuracy:0.982\tPrecision: 1.0\tRecall: 0.933\tF-measure: 0.966\n",
      "Fold: 9\n",
      "Accuracy:0.895\tPrecision: 0.963\tRecall: 0.839\tF-measure: 0.897\n",
      "Fold: 10\n",
      "Accuracy:0.946\tPrecision: 0.913\tRecall: 0.955\tF-measure: 0.933\n",
      "==>Average metric values:\n",
      "Accuracy:0.949\n",
      "Precision: 0.965\n",
      "Recall: 0.905\n",
      "F-measure: 0.933\n",
      "====>project3_dataset2.txt\n",
      "Fold: 1\n",
      "Accuracy:0.596\tPrecision: 0.571\tRecall: 0.381\tF-measure: 0.457\n",
      "Fold: 2\n",
      "Accuracy:0.702\tPrecision: 0.429\tRecall: 0.231\tF-measure: 0.3\n",
      "Fold: 3\n",
      "Accuracy:0.63\tPrecision: 0.571\tRecall: 0.421\tF-measure: 0.485\n",
      "Fold: 4\n",
      "Accuracy:0.674\tPrecision: 0.5\tRecall: 0.333\tF-measure: 0.4\n",
      "Fold: 5\n",
      "Accuracy:0.696\tPrecision: 0.875\tRecall: 0.35\tF-measure: 0.5\n",
      "Fold: 6\n",
      "Accuracy:0.543\tPrecision: 0.4\tRecall: 0.333\tF-measure: 0.364\n",
      "Fold: 7\n",
      "Accuracy:0.63\tPrecision: 0.375\tRecall: 0.462\tF-measure: 0.414\n",
      "Fold: 8\n",
      "Accuracy:0.761\tPrecision: 0.5\tRecall: 0.455\tF-measure: 0.476\n",
      "Fold: 9\n",
      "Accuracy:0.63\tPrecision: 0.455\tRecall: 0.312\tF-measure: 0.37\n",
      "Fold: 10\n",
      "Accuracy:0.761\tPrecision: 0.615\tRecall: 0.571\tF-measure: 0.593\n",
      "==>Average metric values:\n",
      "Accuracy:0.662\n",
      "Precision: 0.529\n",
      "Recall: 0.385\n",
      "F-measure: 0.436\n"
     ]
    }
   ],
   "source": [
    "filenames = ['project3_dataset1.txt', 'project3_dataset2.txt']\n",
    "    \n",
    "for filename in filenames: \n",
    "    print(\"====>\"+filename)\n",
    "    data = pd.read_csv(filename, delimiter=\"\\t\", header=None)\n",
    "    no_of_columns = len(data.columns)\n",
    "    features = np.array(data.iloc[:,0:no_of_columns])\n",
    "    features = np.array([list(x) for x in features])\n",
    "    \n",
    "    string_to_number_map = {}\n",
    "    columns_with_categorical_data = []\n",
    "    \n",
    "    # pre processing\n",
    "    for ele_index, ele in enumerate(features):\n",
    "        for val_index, val in enumerate(ele):\n",
    "            try:\n",
    "                isinstance(float(val), float)\n",
    "            except:\n",
    "                if val_index not in columns_with_categorical_data:\n",
    "                    columns_with_categorical_data.append(val_index)\n",
    "                if val in string_to_number_map:\n",
    "                    ele[val_index] = string_to_number_map[val]\n",
    "                else:\n",
    "                    length = len(string_to_number_map.keys())\n",
    "                    ele[val_index] = length\n",
    "                    string_to_number_map[val] = length\n",
    "        features[ele_index] = ele\n",
    "    features = np.array([[float(y) for y in x] for x in features])\n",
    "    \n",
    "    no_of_folds = 10\n",
    "    feature_sets = np.array_split(features, no_of_folds)\n",
    "    \n",
    "    acc_accuracy = 0\n",
    "    acc_precision = 0\n",
    "    acc_recall = 0\n",
    "    acc_f_measure = 0\n",
    "    \n",
    "    no_of_decision_trees = 5\n",
    "    \n",
    "    for i in range(0, no_of_folds):\n",
    "        \n",
    "        print(\"Fold: \"+str(i+1))    \n",
    "        test_data = feature_sets[i]\n",
    "        temp = [x for x in range(0,10) if x !=i]\n",
    "        train_data = feature_sets[temp[0]]\n",
    "        for i in temp[1:len(temp)]:\n",
    "            train_data = np.concatenate((train_data, feature_sets[i]))\n",
    "        \n",
    "        test_labels = test_data[:,no_of_columns-1]\n",
    "        # removing labels from test data\n",
    "        test_data = [x[0:no_of_columns-1] for x in test_data]\n",
    "        \n",
    "        trees = []\n",
    "        alpha_values = []\n",
    "        \n",
    "        weights = []\n",
    "        weight = 1/len(train_data)\n",
    "        for i in range(0, len(train_data)):\n",
    "            weights.append(weight)\n",
    "            \n",
    "        for i in range(0, no_of_decision_trees):\n",
    "            \n",
    "            while True:\n",
    "\n",
    "                local_train_data = []\n",
    "                # sampling with replacement\n",
    "                indices = list(np.random.choice(len(train_data), int(len(train_data) / no_of_decision_trees), replace = True, p = weights))\n",
    "                for index in indices:\n",
    "                    local_train_data.append(train_data[index])\n",
    "                local_train_data = np.array(local_train_data)\n",
    "                local_test_data = [x[0:no_of_columns-1] for x in local_train_data]\n",
    "                local_train_labels = local_train_data[:,no_of_columns-1]\n",
    "                \n",
    "                tree_map = build_tree(local_train_data, get_entropy(local_train_data), columns_with_categorical_data, \"1.\", {})\n",
    "                \n",
    "                # only root and two children then repeat because we cant prune anything\n",
    "                if len(tree_map) == 3:\n",
    "                    continue\n",
    "                \n",
    "                predictions = []\n",
    "                \n",
    "                for data in local_test_data:\n",
    "                        predictions.append(predict_using_decision_tree(data, tree_map, columns_with_categorical_data))\n",
    "                        \n",
    "                accuracy, precision, recall, f_measure, miss_classification_rate = evaluate(local_train_labels, predictions)\n",
    "                \n",
    "                if int(accuracy) == 1: \n",
    "                    # post pruning\n",
    "                    while True:            \n",
    "                        leaf_nodes = [x for x in tree_map if tree_map[x] == 1 or tree_map[x] == 0]\n",
    "                        level = [len(x) for x in leaf_nodes]\n",
    "                        max_level = level.index(max(level))\n",
    "                        node_to_prune = leaf_nodes[max_level]\n",
    "                        parent_node = node_to_prune[0:len(node_to_prune)-1]\n",
    "                        if parent_node == \"1.\":\n",
    "                            break\n",
    "                        leaf_node_value = tree_map[node_to_prune]\n",
    "                        del tree_map[node_to_prune]\n",
    "                        tree_map[parent_node] = leaf_node_value\n",
    "\n",
    "                        predictions = []\n",
    "\n",
    "                        for data in local_train_data:\n",
    "                            predictions.append(predict_using_decision_tree(data, tree_map, columns_with_categorical_data))\n",
    "\n",
    "                        accuracy, precision, recall, f_measure, miss_classification_rate = evaluate(local_train_labels, predictions)\n",
    "\n",
    "                        if accuracy < 1:\n",
    "                            break\n",
    "                \n",
    "                error = 0\n",
    "                \n",
    "                # calculating error\n",
    "                for index, weight in enumerate(weights):\n",
    "                    if index in indices:\n",
    "                        position = indices.index(index)\n",
    "                        if predictions[position] != local_train_labels[position]:\n",
    "                            error = error + weights[index]\n",
    "                # total of all weights is 1, so no need to divide anything\n",
    "                \n",
    "                if error <= 0.5:\n",
    "                    break\n",
    "            \n",
    "            trees.append(tree_map)\n",
    "            alpha = (1/2) * math.log((1-error)/error)\n",
    "            alpha_values.append(alpha)\n",
    "            \n",
    "            # updating weights\n",
    "            for index, weight in enumerate(weights):\n",
    "                if index in indices:\n",
    "                    position = indices.index(index)\n",
    "                    if predictions[position] != local_train_labels[position]:\n",
    "                        weights[index] = weights[index] * math.sqrt((1-error)/error)\n",
    "                    else:\n",
    "                        weights[index] = weights[index] * math.sqrt(error/(1-error))\n",
    "            \n",
    "            # normalizing weights\n",
    "            total = np.sum(weights)\n",
    "            weights = [x/total for x in weights]\n",
    "                \n",
    "        predictions = []\n",
    "        \n",
    "        for data in test_data:\n",
    "            prediction_weight_map = {}\n",
    "            for i in range(0, no_of_decision_trees):\n",
    "                prediction = predict_using_decision_tree(data, trees[i], columns_with_categorical_data)\n",
    "                if prediction in prediction_weight_map:\n",
    "                    prediction_weight_map[prediction] = prediction_weight_map[prediction] + alpha_values[i]\n",
    "                else:\n",
    "                    prediction_weight_map[prediction] = alpha_values[i]\n",
    "            max_weight = -1\n",
    "            prediction = -1\n",
    "            for key in prediction_weight_map:\n",
    "                weight = prediction_weight_map[key]\n",
    "                if max_weight == -1:\n",
    "                    max_weight = weight\n",
    "                    prediction = key\n",
    "                else:\n",
    "                    if weight > max_weight:\n",
    "                        max_weight = weight\n",
    "                        prediction = key\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        accuracy, precision, recall, f_measure, miss_classification_rate = evaluate(test_labels, predictions)\n",
    "        print(\"Accuracy:\"+str(accuracy)+\"\\tPrecision: \"+str(precision)+\"\\tRecall: \"+str(recall)+\"\\tF-measure: \"+str(f_measure))\n",
    "        acc_accuracy += accuracy\n",
    "        acc_precision += precision\n",
    "        acc_recall += recall\n",
    "        acc_f_measure += f_measure\n",
    "        \n",
    "    print(\"==>Average metric values:\")\n",
    "    print(\"Accuracy:\"+str(round(acc_accuracy/no_of_folds, 3)))\n",
    "    print(\"Precision: \"+str(round(acc_precision/no_of_folds, 3)))\n",
    "    print(\"Recall: \"+str(round(acc_recall/no_of_folds, 3)))\n",
    "    print(\"F-measure: \"+str(round(acc_f_measure/no_of_folds, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
